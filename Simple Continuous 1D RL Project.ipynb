{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Continuous 1D Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epsilonGreedyDiscrete(epsilon, Q, state):\n",
    "    '''\n",
    "    epsilon is probability of taking a random move\n",
    "    '''\n",
    "    vM = validMoves(state)\n",
    "    if epsilon > random.uniform(0,1):\n",
    "        # take random action\n",
    "        move = random.choice(vM)\n",
    "#         print('took random move {}'.format(move))\n",
    "    else:\n",
    "        # take greedy action\n",
    "        Qs = np.array([Q.get(smt(state,a),0) for a in vM])\n",
    "        move = vM[np.argmax(Qs)]\n",
    "#         print('took GREEDY move {}'.format(move))\n",
    "    return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "    '''Abstract class representing a reinforcement learning agent'''\n",
    "    def __init__(self):        \n",
    "        self.Q = lambda x: 0\n",
    "        \n",
    "    @abstractmethod\n",
    "    def train(X, Qt, nIterations):\n",
    "        '''\n",
    "        Train the agent based on the data provided\n",
    "        Input consists of [s,a,r] and output data consists of [q]\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def Q(state, action):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def act(state):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def randomAction(state):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def greedyAction(state):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Environment(ABC):\n",
    "    '''Abstract class representing a reinforcement learning environment'''\n",
    "    def __init__(self, initialState):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def reinforce(state):\n",
    "        '''\n",
    "        Reinforcement function for any valid state\n",
    "        Should be deterministic and not stateful\n",
    "        '''\n",
    "        return 0\n",
    "    \n",
    "    @abstractmethod\n",
    "    def validActions(state):\n",
    "        '''\n",
    "        Returns iterable of valid discrete from any valid state\n",
    "        Should be deterministic and not stateful\n",
    "        '''\n",
    "        return []\n",
    "    \n",
    "    @abstractmethod\n",
    "    def step(self, state, action):\n",
    "        '''\n",
    "        Advance the state of the Environment by taking an action\n",
    "        Should be deterministic and **NOT** stateful\n",
    "        step does NOT update the state of the Environment\n",
    "        '''\n",
    "        return self.state\n",
    "    \n",
    "    def run(agent, nEpochs, nSteps):\n",
    "        '''\n",
    "        Let the agent loose in the environment for nEpochs epochs of size nSteps steps\n",
    "        Should use a subclass of Agent designed for the same problem\n",
    "            as the subclass for Environment\n",
    "        Will generate (nEpochs * nSteps) samples\n",
    "        '''\n",
    "        inputs = []\n",
    "        qs = []\n",
    "        for i in range(nEpochs):\n",
    "            for j in range(nSteps):\n",
    "                action = agent.act(self.state)\n",
    "                newState = step(action)\n",
    "                inputs.append([state,action])\n",
    "                # temporal difference error\n",
    "                qs.append(agent.Q(state,action)\n",
    "                               + agent.learningRate*(reinforce(newState) - agent.Q(state,action)))\n",
    "        return inputs,qs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    return (x - np.mean(x,axis=1)) / np.std(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DiscreteQNetAgent(Agent):\n",
    "    '''\n",
    "    Concrete subclass of Agent\n",
    "    RL Agent with discrete action space\n",
    "    Q function is a PyTorch Neural Network\n",
    "    '''\n",
    "    def __init__(self, nInputs, nHidden, nOutputs, QnetClass, validActionsF, reinforcementF, useCuda=True):\n",
    "        self.Qnet = QNetClass(nInputs, nHidden, nOutputs)\n",
    "        self.useCuda = useCuda\n",
    "        if torch.cuda.is_available() and useCuda:\n",
    "            self.Qnet = self.Qnet.cuda()\n",
    "        self.criterion = torch.nn.MSELoss(size_average=False)\n",
    "        self.optimizer = torch.optim.SGD(self.Qnet.parameters(), lr=1e-4)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 30)\n",
    "        self.lossTrace = []\n",
    "        self.validActions = validActionsF\n",
    "        self.reinforce = reinforcementF\n",
    "    \n",
    "    def Q(state, action):\n",
    "        return Qnet(torch.cat(state,action))\n",
    "    \n",
    "    def train(X, Qt, nIterations):\n",
    "        '''\n",
    "        Trains the agent's Q function on the provided epochs of data. \n",
    "        X = input samples (one s/a/r sample per row)\n",
    "        Qt = output samples (one q value per row)\n",
    "        X and Qt should have the same number of rows (X.shape[1] == Qt.shape[1])\n",
    "        '''\n",
    "        # standardize x values\n",
    "        X = standardize(X) \n",
    "\n",
    "        # X and Qt are likely not Tensors yet\n",
    "        x = Variable(torch.FloatTensor(X))\n",
    "        y = Variable(torch.FloatTensor(Qt), requiresGrad=False)\n",
    "        \n",
    "        if self.useCuda:\n",
    "            x = x.type(torch.cuda.FloatTensor)\n",
    "            y = x.type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        lossTrace = []\n",
    "        for t in range(nIterations):\n",
    "            # Forward pass: Compute predicted y by passing x to the model\n",
    "            y_pred = self.Qnet(x)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = self.criterion(y_pred, y)\n",
    "            lossTrace.append(loss.data[0])\n",
    "\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # optimizer.step()\n",
    "            self.scheduler.step()\n",
    "        self.lossTrace.append(lossTrace)\n",
    "    \n",
    "    def randomAction(state):\n",
    "        import random\n",
    "        return random.choice(validActions(state))\n",
    "    \n",
    "    def greedyAction(state):\n",
    "        qValues = np.array([Q(state,action) for action in validActions(state)])\n",
    "        return np.argmax(qValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Reltan(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,n,h,o):\n",
    "        super(Model, self).__init__()\n",
    "        self.mods = torch.nn.ModuleList([])\n",
    "        self.mods.append(torch.nn.Linear(n, h))\n",
    "        self.mods.append(torch.nn.ReLU())\n",
    "        self.mods.append(torch.nn.Tanh())\n",
    "        self.mods.append(torch.nn.ReLU())\n",
    "        self.mods.append(torch.nn.Tanh())\n",
    "        self.mods.append(torch.nn.ReLU())\n",
    "        self.mods.append(torch.nn.Linear(h,o))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #return self.fc(x) # it was just x there\n",
    "        for module in self.mods:\n",
    "            x = module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneDimEnv(Environment):\n",
    "    '''Abstract class representing a reinforcement learning environment'''\n",
    "    def __init__(self, initialState):\n",
    "        pass\n",
    "    \n",
    "    def reinforce(state):\n",
    "        '''\n",
    "        Reinforcement function for any valid state\n",
    "        Should be deterministic and not stateful\n",
    "        '''\n",
    "        return 0\n",
    "    \n",
    "    def validActions(state):\n",
    "        '''\n",
    "        Returns iterable of valid discrete from any valid state\n",
    "        Should be deterministic and not stateful\n",
    "        '''\n",
    "        return []\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        '''\n",
    "        Advance the state of the Environment by taking an action\n",
    "        Should be deterministic and **NOT** stateful\n",
    "        step does NOT update the state of the Environment\n",
    "        '''\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```\n",
    "\n",
    "def trainQ(epoch, learningRate, epsilonDecayFactor, validMovesF, makeMoveF):\n",
    "    startState = [[1, 2, 3], [], []]\n",
    "    epsilon = 1\n",
    "    steps = 0\n",
    "    reinf = -1\n",
    "    stepsList = []\n",
    "    Q = {}\n",
    "    s = startState\n",
    "    a = epsilonGreedy(epsilon,Q,s)\n",
    "    \n",
    "    \n",
    "    for rep in range(nRepetitions):\n",
    "        epsilon = epsilon * epsilonDecayFactor\n",
    "        while not isGoal(s,2):\n",
    "            if steps > 0:\n",
    "                Q[smt(sold,aold)] = Q.get(smt(sold,aold),0) + learningRate * (reinf- Q.get(smt(sold,aold),0) + Q.get(smt(s,a),0))\n",
    "            sold,aold = s,a\n",
    "            s = makeMove(sold,aold)\n",
    "            a = epsilonGreedy(epsilon, Q, s)\n",
    "            steps+=1\n",
    "        Q[smt(s,a)] = 0\n",
    "        if steps > 0:\n",
    "            Q[smt(sold,aold)] = reinf\n",
    "        s = startState\n",
    "        a = epsilonGreedy(epsilon, Q, s)\n",
    "        stepsList.append(steps)\n",
    "        steps = 0\n",
    "    return Q,stepsList\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
